{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPuhXNq+TNdt9L3dhZgUoGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bradkim1/ScaleMLPrototype/blob/main/ScaleMLPrototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scale ML prototype with Large Scale Data\n",
        "In this notebook, I developed a scalable fraud detection workflow using a modular Python class capable of handling large datasets. The pipeline encapsulates preprocessing, feature engineering, and model training using RandomForestClassifier, GradientBoostingClassifier, or XGBClassifier, with support for outlier removal, PCA-based dimensionality reduction of high-dimensional V features, and aggregated statistical features.\n",
        "\n",
        "To prevent memory overload during prediction on large datasets, the workflow supports chunked processing—reading and transforming data in smaller batches to avoid loading the entire dataset into memory at once. This chunk-based approach enables efficient training and inference even in memory-constrained environments like Colab.\n",
        "\n",
        "For deployment readiness, the pipeline guarantees consistent preprocessing between training and inference and writes predictions—including probabilities—to disk. While this prototype processes training data as full DataFrames, it is structured to be extended easily for streaming or distributed prediction scenarios with minimal changes.\n",
        "\n",
        "Original datasets source:\n",
        "https://www.kaggle.com/competitions/ieee-fraud-detection/data\n",
        "\n",
        "Data file used in this notebook: https://drive.google.com/drive/folders/1XVqZ1psVwXygFlhLsOjvrhm9XM0wKlQ2?dmr=1&ec=wgc-drive-globalnav-goto"
      ],
      "metadata": {
        "id": "9VPNLM7gbJU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n"
      ],
      "metadata": {
        "id": "lsUEOmyXa3Dc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lkOu8zY-a0y9"
      },
      "outputs": [],
      "source": [
        "class AdvancedMLPipeline:\n",
        "    def __init__(self, model_type='rf', n_components=10, remove_outliers=True):\n",
        "        # Initialize all components: model, imputers, encoder, PCA\n",
        "        model_map = {\n",
        "            'gb': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),\n",
        "            'rf': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'xgb': XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "        }\n",
        "        self.model = model_map[model_type]\n",
        "        #self.model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.imputer_num = SimpleImputer(strategy='mean')\n",
        "        self.imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "        self.encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        self.pca = PCA(n_components=n_components)\n",
        "        self.remove_outliers = remove_outliers\n",
        "        self.cat_columns = []\n",
        "        self.feature_columns = []\n",
        "\n",
        "    def _add_features(self, df):\n",
        "        # Create time features from TransactionDT\n",
        "        if 'TransactionDT' in df.columns:\n",
        "            df['hour'] = pd.to_datetime(df['TransactionDT'], unit='s', errors='coerce').dt.hour.astype('Int8')\n",
        "            df['day'] = pd.to_datetime(df['TransactionDT'], unit='s', errors='coerce').dt.dayofweek.astype('Int8')\n",
        "            df.drop(columns=['TransactionDT'], inplace=True)\n",
        "\n",
        "        # Group-level aggregation features\n",
        "        c_cols = [col for col in df.columns if col.startswith('C')]\n",
        "        d_cols = [col for col in df.columns if col.startswith('D')]\n",
        "        v_cols = [col for col in df.columns if col.startswith('V')]\n",
        "\n",
        "        df['C_sum'] = df[c_cols].sum(axis=1)\n",
        "        df['D_missing'] = df[d_cols].isnull().sum(axis=1)\n",
        "        df['V_mean'] = df[v_cols].mean(axis=1)\n",
        "        return df\n",
        "\n",
        "    def _remove_outliers(self, df, col='TransactionAmt'):\n",
        "        # Remove outliers from TransactionAmt using IQR\n",
        "        if col in df.columns:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 1.5 * IQR\n",
        "            upper = Q3 + 1.5 * IQR\n",
        "            before = df.shape[0]\n",
        "            df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
        "            after = df.shape[0]\n",
        "            print(f\"Removed {before - after} outliers from {col}\")\n",
        "        return df\n",
        "\n",
        "    def fit_model_on_chunk(self, df, label_column='isFraud'):\n",
        "        df = self._add_features(df)\n",
        "\n",
        "        if self.remove_outliers:\n",
        "            df = self._remove_outliers(df)\n",
        "\n",
        "        # Convert known categoricals to string\n",
        "        known_cats = ['ProductCD', 'card4', 'card6', 'DeviceType', 'DeviceInfo',\n",
        "                      'M1','M2','M3','M4','M5','M6','M7','M8','M9']\n",
        "        for col in known_cats:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str)\n",
        "\n",
        "        # Identify categorical and numeric columns\n",
        "        self.cat_columns = [col for col in df.columns if df[col].dtype == 'object' and col != label_column]\n",
        "        num_cols = [col for col in df.columns if col not in self.cat_columns + [label_column, 'TransactionID']]\n",
        "\n",
        "        # Impute missing values\n",
        "        if self.cat_columns:\n",
        "            df[self.cat_columns] = self.imputer_cat.fit_transform(df[self.cat_columns])\n",
        "        if num_cols:\n",
        "            df[num_cols] = self.imputer_num.fit_transform(df[num_cols])\n",
        "\n",
        "        # One-hot encode categorical features\n",
        "        encoded = self.encoder.fit_transform(df[self.cat_columns])\n",
        "        encoded_df = pd.DataFrame(encoded, columns=self.encoder.get_feature_names_out(self.cat_columns), index=df.index)\n",
        "\n",
        "        # Replace original categoricals with encoded version\n",
        "        df = df.drop(columns=self.cat_columns)\n",
        "        df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "        # Scale numeric columns\n",
        "        if num_cols:\n",
        "            df[num_cols] = self.scaler.fit_transform(df[num_cols])\n",
        "\n",
        "        # Apply PCA on V-columns\n",
        "        v_cols = [col for col in df.columns if col.startswith('V')]\n",
        "        if v_cols:\n",
        "            pca_trans = self.pca.fit_transform(df[v_cols])\n",
        "            pca_df = pd.DataFrame(pca_trans, columns=[f'V_PCA_{i}' for i in range(pca_trans.shape[1])], index=df.index)\n",
        "            df = df.drop(columns=v_cols)\n",
        "            df = pd.concat([df, pca_df], axis=1)\n",
        "\n",
        "        # Save the final feature columns\n",
        "        self.feature_columns = [col for col in df.columns if col not in [label_column, 'TransactionID']]\n",
        "\n",
        "        # Train model\n",
        "        X = df[self.feature_columns]\n",
        "        y = df[label_column]\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "    def transform_for_predict(self, df):\n",
        "        df = self._add_features(df)\n",
        "\n",
        "        # Fill missing categorical values\n",
        "        for col in self.cat_columns:\n",
        "            if col not in df.columns:\n",
        "                df[col] = \"missing\"\n",
        "        df[self.cat_columns] = self.imputer_cat.transform(df[self.cat_columns])\n",
        "\n",
        "        # Impute numeric\n",
        "        num_cols = [col for col in df.columns if col not in self.cat_columns + ['TransactionID']]\n",
        "        df[num_cols] = self.imputer_num.transform(df[num_cols])\n",
        "\n",
        "        # Encode categoricals\n",
        "        encoded = self.encoder.transform(df[self.cat_columns])\n",
        "        encoded_df = pd.DataFrame(encoded, columns=self.encoder.get_feature_names_out(self.cat_columns), index=df.index)\n",
        "\n",
        "        df = df.drop(columns=self.cat_columns)\n",
        "        df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "        # Scale numeric\n",
        "        if num_cols:\n",
        "            df[num_cols] = self.scaler.transform(df[num_cols])\n",
        "\n",
        "        # Apply PCA to V columns\n",
        "        v_cols = [col for col in df.columns if col.startswith('V')]\n",
        "        if v_cols:\n",
        "            pca_trans = self.pca.transform(df[v_cols])\n",
        "            pca_df = pd.DataFrame(pca_trans, columns=[f'V_PCA_{i}' for i in range(pca_trans.shape[1])], index=df.index)\n",
        "            df = df.drop(columns=v_cols)\n",
        "            df = pd.concat([df, pca_df], axis=1)\n",
        "\n",
        "        # Align columns with training\n",
        "        for col in self.feature_columns:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 0\n",
        "        df = df[self.feature_columns]\n",
        "        return df\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred = self.model.predict(X)\n",
        "        y_proba = self.model.predict_proba(X)[:, 1]\n",
        "        print(classification_report(y_true, y_pred))\n",
        "        print(\"ROC AUC:\", roc_auc_score(y_true, y_proba))\n",
        "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    def plot_roc_curve(self, X, y_true):\n",
        "        from sklearn.metrics import roc_curve, auc\n",
        "        y_proba = self.model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_precision_recall(self, X, y_true):\n",
        "        from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "        y_proba = self.model.predict_proba(X)[:, 1]\n",
        "        precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
        "        avg_precision = average_precision_score(y_true, y_proba)\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure()\n",
        "        plt.plot(recall, precision, label=f'Avg Precision = {avg_precision:.2f}')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def get_metrics(self, X, y_true):\n",
        "        from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "        y_pred = self.model.predict(X)\n",
        "        y_proba = self.model.predict_proba(X)[:, 1]\n",
        "        return {\n",
        "            'f1_score': f1_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred),\n",
        "            'recall': recall_score(y_true, y_pred),\n",
        "            'roc_auc': roc_auc_score(y_true, y_proba)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading datasets from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhkURoUMfrt2",
        "outputId": "5346ce61-f23b-4ee6-abed-49345a09a0b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation\n",
        "# Load and clean training data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML_Datasets/train_merged.csv\")\n",
        "\n",
        "# Optional: drop very high-null or low-variance columns\n",
        "drop_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1 or df[col].isnull().mean() > 0.95]\n",
        "df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "# Train/validation split\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, stratify=df['isFraud'], random_state=42)\n",
        "\n",
        "# Initialize and train pipeline\n",
        "pipeline = AdvancedMLPipeline(model_type='rf', n_components=15, remove_outliers=True)\n",
        "pipeline.fit_model_on_chunk(df_train.copy(), label_column='isFraud')\n",
        "\n",
        "# Evaluate on validation set\n",
        "X_val = pipeline.transform_for_predict(df_val.drop(columns=['isFraud']))\n",
        "pipeline.evaluate(X_val, df_val['isFraud'])\n",
        "\n",
        "# Extra metrics\n",
        "#pipeline.plot_roc_curve(X_val, df_val['isFraud'])\n",
        "#pipeline.plot_precision_recall(X_val, df_val['isFraud'])\n",
        "metrics = pipeline.get_metrics(X_val, df_val['isFraud'])\n",
        "print(metrics)\n",
        "\n",
        "# Load test data and make predictions\n",
        "# Prepare chunked test prediction\n",
        "chunk_size = 100_000\n",
        "top_n = 1000  # Change this to control how many top frauds to save\n",
        "reader = pd.read_csv(\"/content/drive/MyDrive/ML_Datasets/test_merged.csv\", chunksize=chunk_size)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, chunk in enumerate(reader):\n",
        "    print(f\" Processing chunk {i+1}\")\n",
        "    # Drop columns not used in training\n",
        "    chunk = chunk.drop(columns=[col for col in chunk.columns if col not in df_train.columns and col != 'TransactionID'], errors='ignore')\n",
        "\n",
        "    # Transform and predict\n",
        "    X_chunk = pipeline.transform_for_predict(chunk.copy())\n",
        "    y_prob = pipeline.model.predict_proba(X_chunk)[:, 1]\n",
        "    y_pred = pipeline.model.predict(X_chunk)\n",
        "\n",
        "    # Create prediction DataFrame\n",
        "    result_chunk = pd.DataFrame({\n",
        "        'TransactionID': chunk['TransactionID'],\n",
        "        'isFraud': y_pred,\n",
        "        'fraudProbability': y_prob\n",
        "    })\n",
        "\n",
        "    results.append(result_chunk)\n",
        "    del chunk, X_chunk, y_prob, y_pred, result_chunk\n",
        "    gc.collect()\n",
        "\n",
        "# test data likely contains >1M+ rows\n",
        "# After one-hot encoding, the column count can explode\n",
        "# Then PCA transformation and alignment to >multiple_thousand+ features — done on >1M+ rows can explode RAM!!!\n",
        "\n",
        "# Instead of transforming the whole test set at once, process it in parts and collect predictions gradually\n",
        "# transform_for_predict() step is exploding RAM due to:\n",
        "# One-hot encoding on many categorical columns\n",
        "# Keeping hundreds of V columns + PCA transform on them\n",
        "# No chunking — it's transforming 500,000+ rows at once\n",
        "\n",
        "# Combine all chunked results and save\n",
        "# Save predictions to file\n",
        "final_predictions = pd.concat(results)\n",
        "\n",
        "# Sort by highest fraud probability and keep top N\n",
        "top_frauds = final_predictions.sort_values(by='fraudProbability', ascending=False).head(top_n)\n",
        "\n",
        "final_predictions.to_csv(\"/content/drive/MyDrive/ML_Datasets/test_predictions.csv\", index=False)\n",
        "print(\"Top {top_n} predictions saved to: top_fraud_test_predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeYv_UUga_qJ",
        "outputId": "6edc96ee-08cc-4b36-978b-b7e552a04411"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 53211 outliers from TransactionAmt\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99    113975\n",
            "           1       0.97      0.36      0.52      4133\n",
            "\n",
            "    accuracy                           0.98    118108\n",
            "   macro avg       0.97      0.68      0.76    118108\n",
            "weighted avg       0.98      0.98      0.97    118108\n",
            "\n",
            "ROC AUC: 0.9261526358685571\n",
            "Confusion Matrix:\n",
            " [[113928     47]\n",
            " [  2652   1481]]\n",
            "{'f1_score': 0.5232291114644055, 'precision': 0.9692408376963351, 'recall': 0.35833534962496977, 'roc_auc': np.float64(0.9261526358685571)}\n",
            " Processing chunk 1\n",
            " Processing chunk 2\n",
            " Processing chunk 3\n",
            " Processing chunk 4\n",
            " Processing chunk 5\n",
            " Processing chunk 6\n",
            "Top {top_n} predictions saved to: top_fraud_test_predictions.csv\n"
          ]
        }
      ]
    }
  ]
}